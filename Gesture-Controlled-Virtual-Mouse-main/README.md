AIVirtualMouse simplifies human-computer interaction using hand gestures and voice commands, eliminating the need for direct contact. All I/O operations are virtually controlled through both static and dynamic hand gestures combined with a voice assistant. This project employs advanced Machine Learning and Computer Vision algorithms to seamlessly recognize hand gestures and voice commands, without requiring additional hardware. It utilizes CNN models implemented by MediaPipe, running on pybind11. The system consists of two modules: one that detects hands directly using MediaPipe Hand detection, and another that uses gloves of any uniform color. Currently, it operates on the Windows platform.
